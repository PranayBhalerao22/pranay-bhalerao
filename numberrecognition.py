# -*- coding: utf-8 -*-
"""NumberRecognition.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1RQ5g828t3-pmXTALdfxRzgNia_UzKpw9

#3RD TASK
"""

# Commented out IPython magic to ensure Python compatibility.
import tensorflow as tf
from tensorflow import keras
import pandas as pd
import matplotlib.pyplot as plt
# %matplotlib inline
import numpy as np

"""Load the MNIST dataset"""

(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()

len(x_train)

len(x_test)

# Finding the shape of individual smaple
x_train[0].shape

x_train[0]

"""See this images"""

plt .matshow(x_train[0])

y_train[0]

# Show First 5 data
y_train[:5]

x_train.shape

# Sacle the data so that the values are from 0 - 1
x_train = x_train / 255
x_test = x_test / 255

for i in range(9):
  plt.subplot(330+1+i)
  plt.imshow(x_train[i],cmap=plt.cm.coolwarm)
  plt.axis(False)

x_train[0]

# FLattening the train and test data
x_train_flattened = x_train.reshape(len(x_train), 28*28)
x_test_flattened = x_test.reshape(len(x_test), 28*28)

x_train_flattened.shape

"""Create a simple neural network in keras"""

#Sequential create a stack of layers
model = keras.Sequential([
    keras.layers.Dense(10, input_shape=(784,), activation='sigmoid')
])

# Optimizer will help in backproagation to reach better global optima
model.compile(
    optimizer='adam',
    loss='sparse_categorical_crossentropy',
    metrics=['accuracy']
)

# Does the training
model.fit(x_train_flattened, y_train, epochs=5)

"""Evaluate the accuracy on test data"""

model.evaluate(x_test_flattened, y_test)

"""Sample prediction"""

#Show the image
plt.matshow(x_test[0])

#imposinglayers of neural network by using tensorflow
from keras.models import Sequential
from keras.layers import Dense, Conv2D, MaxPooling2D, LSTM, Dropout
network=tf.keras.models.Sequential([tf.keras.layers.Flatten(input_shape=(28,28)),
                                    tf.keras.layers.Dense(128,activation="relu"),
                                    tf.keras.layers.Dense(10,activation=tf.nn.softmax)])

network.compile(optimizer=tf.keras.optimizers.Adam(),
                loss="sparse_categorical_crossentropy", metrics=["accuracy"])

#performance of the network model evaluation
network.evaluate(x_test,y_test)

network.fit(x_train,y_train,epochs=5)

y_predict=[]
predict=network.predict(x_test)
for i in predict:
  y_predict.append(np.argmax(i))

y_predict=np.array(y_predict)
y_predict

y_test

for i in range(0,15):
  plt.figure(figsize=(16,12))
  plt.subplot(1,15,i+1)
  plt.imshow(x_test[i],cmap=plt.cm.coolwarm)
  plt.xlabel(y_predict[i])
  plt.title(y_test[i])

#representing in the tabular form
real_val=y_test
predicted=y_predict
final=pd.DataFrame({"real_val ":real_val,"predicted ":predicted})
final.head(40)

#predicted value
predict=network.predict(x_test)
print(np.argmax(predict[5]))

#real value
plt.imshow(x_test[5],cmap=plt.cm.coolwarm)
plt.axis(False)

#Make the predictions
y_predicted = model.predict(x_test_flattened)
y_predicted[0]

# Find the maximum value using numpy
np.argmax(y_predicted[0])

# converting y_predicted from whole numbers to integers
# so that we can use it in confusion matrix
# In short we are argmaxing the entire prediction
y_predicted_labels = [np.argmax(i) for i in y_predicted]
y_predicted_labels[:5]

"""Using confusion matrix for validation"""

cm = tf.math.confusion_matrix(labels=y_test, predictions=y_predicted_labels)
cm

"""Using seaborn to make confusion matrix look good"""

import seaborn as sn
plt.figure(figsize = (10,7))
sn.heatmap(cm, annot=True, fmt='d')
plt.xlabel('Predicted')
plt.ylabel('Truth')

"""Adding a hidden layer"""

# Sequential create a stack of layers
# Create a hidden layer with 100 neurons and relu activation
model = keras.Sequential([
    keras.layers.Dense(100, input_shape=(784,), activation='relu'),
    keras.layers.Dense(10, activation='sigmoid')
])

# Optimizer will help in backproagation to reach better global optima
model.compile(
    optimizer='adam',
    loss='sparse_categorical_crossentropy',
    metrics=['accuracy']
)

# Does the training
model.fit(x_train_flattened, y_train, epochs=5)

"""Evaluate the accuracy on test set"""

model.evaluate(x_test_flattened, y_test)

"""Now we can observe that by adding a hidden layer the accuracy increased from 92% to 97%.

Using confusion matrix for validation
"""

y_predicted = model.predict(x_test_flattened)
y_predicted_labels = [np.argmax(i) for i in y_predicted]

cm = tf.math.confusion_matrix(labels=y_test, predictions=y_predicted_labels)

import seaborn as sn
plt.figure(figsize = (10,7))
sn.heatmap(cm, annot=True, fmt='d')
plt.xlabel('Predicted')
plt.ylabel('Truth')

"""Compared to the previous confusion matrix the wrong predictions has gone down. We can see that the diagonal values has increased and the values in black cells has gone down. There are more 0 valued black cells, meaning correct predictions."""